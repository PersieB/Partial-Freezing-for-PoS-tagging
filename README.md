In extending the capability of a pre-trained model to a new but related task, it is important to ensure that we prevent catastrophic forgetting. By partially freezing some layers of a pre-trained model, we can preserve existing knowledge from the pre-trained models while adapting specific layers to the new task. It is also advantageous in situations where there is no large amount of data for the new task as well as limited computational resources. The notebook compares the two approaches - using a baseline distilbert model as compared to a partially frozen one. DistilBert is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40\% fewer parameters than bert-base-uncased and runs 60\% faster while preserving over 95\% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.

The notebook titled "Partial_Freezing_PoS" is well documented and comprehensive. 
